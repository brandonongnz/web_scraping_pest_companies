{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Written by: Brandon Ong\n",
    "\n",
    "### Project: Web scraping script for scraping company infomation from California pest companies directory.\n",
    "\n",
    "#### Website: http://pcoc.officialbuyersguide.net/\n",
    "\n",
    "#### Methodology:\n",
    "\n",
    "#### 1) Before writing code, inspect the website, starting from home page, to determine logical (manual) steps required to get to each company's info. At the same time, inspect html code and locate elements that you need for each step. \n",
    "\n",
    "#### 2) Steps required:\n",
    "   #### a. From home page, click into first category (total of 9 categories)\n",
    "   #### b. For each company listed, click into company's name to access full company info (Name, phone and email)\n",
    "   #### c. Go through all pages (if applicable) to cover all companies within each category\n",
    "   #### d. Repeat a-c for all 9 categories, and skip category if listing is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "home_url = \"http://pcoc.officialbuyersguide.net/\"\n",
    "\n",
    "def link_content(link):\n",
    "    '''\n",
    "    Retrieve html and return it's full content\n",
    "    '''\n",
    "    r = requests.get(link, timeout=5)\n",
    "    soup = BeautifulSoup(r.content)\n",
    "    return soup\n",
    "\n",
    "# retrieve home page content \n",
    "home_content = link_content(home_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Use inspect element on your browser \n",
    "\n",
    "#### The 'a href' tags containing link to company listings in each category can be found within div tags grouped under class='HomeCategory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://pcoc.officialbuyersguide.net//SearchResults?categories=1', 'http://pcoc.officialbuyersguide.net//SearchResults?categories=2', 'http://pcoc.officialbuyersguide.net//SearchResults?categories=4', 'http://pcoc.officialbuyersguide.net//SearchResults?categories=3', 'http://pcoc.officialbuyersguide.net//SearchResults?categories=5', 'http://pcoc.officialbuyersguide.net//SearchResults?categories=6', 'http://pcoc.officialbuyersguide.net//SearchResults?categories=18', 'http://pcoc.officialbuyersguide.net//SearchResults?categories=19', 'http://pcoc.officialbuyersguide.net//SearchResults?categories=21']\n"
     ]
    }
   ],
   "source": [
    "def category_url(home_content):  \n",
    "    '''\n",
    "    Retrieve all category a hrefs, \n",
    "    construct full urls, \n",
    "    and append it to a list for later usage\n",
    "    '''\n",
    "    cat_links = []\n",
    "    cat_tags = [div.a for div in home_content.findAll('div',{'class' : 'HomeCategory'})]\n",
    "    for param in cat_tags:\n",
    "        cat_param = param.get(\"href\")\n",
    "        cat_url = home_url + cat_param\n",
    "        cat_links.append(cat_url)\n",
    "    return cat_links \n",
    "\n",
    "category_links = category_url(home_content)\n",
    "print category_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each page of listings, the company link can be found inside href tags grouped under class=\"companyNameLink\", nested within div class=\"ListingNameAddress\"\n",
    "\n",
    "#### For categories with multiple pages of listings, pagination is possible by simply adding a parameter \"&pg={pagenumber}\" at the end of each category url.\n",
    "\n",
    "#### For example, page 2 of category 1:\n",
    "\n",
    "#### \"http://pcoc.officialbuyersguide.net//SearchResults?categories=1&pg=2\"\n",
    "\n",
    "#### To scrape through all company links in each category we will need a function to iterate through all pages, scrape each company's info and stop when it hits a page with empty listings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_listings(link, num):\n",
    "    '''\n",
    "    Main function for scraping, this is a recursive function that will do the following:\n",
    "    1) Access a category page - category link given as argument\n",
    "    2) Extract all company links starting on page 1\n",
    "    3) For each company link, extract individual company's information using extract_info() function\n",
    "    4) Append company information into a list assigned as \"co_list\"\n",
    "    5) Iterate through all pages and repeat step 2, 3 and 4 until page returns empty listing\n",
    "    6) Finally, return full list of companies and their infomation \n",
    "    '''\n",
    "    full_link = link + \"&pg=\" + str(num)\n",
    "    page_content = link_content(full_link)\n",
    "    company_tags = [div.a for div in page_content.findAll('div',{'class' : 'ListingNameAddress'})]\n",
    "    \n",
    "    co_list = [] #Final list that will house information of all companies scraped\n",
    "    \n",
    "    if not company_tags: #for handling pages with no company listings\n",
    "        return \n",
    "    else:\n",
    "        for a in company_tags:\n",
    "            company_param = a.get(\"href\")\n",
    "            company_url = home_url + company_param \n",
    "            co_info = extract_info(company_url)\n",
    "            co_list.append(co_info)\n",
    "        num+=1\n",
    "        listings = extract_listings(link, num)\n",
    "        if listings:\n",
    "            co_list.extend(listings)\n",
    "        return co_list\n",
    "    \n",
    "def extract_info(co_link):\n",
    "    '''\n",
    "    This function is used to extract the required information\n",
    "    from individual company page\n",
    "    *Added exceptions for handling missing information\n",
    "    '''\n",
    "    company_content = link_content(co_link)\n",
    "     \n",
    "    try:\n",
    "        name = company_content.find('div',{'class' : 'ListingPageNameAddress NONE'}).h1.get_text()\n",
    "    except:\n",
    "        name = None\n",
    "    \n",
    "    try:\n",
    "        phone = company_content.find('span',{'id' : 'hiddenSaTp'}).get_text()\n",
    "    except:\n",
    "        phone = None\n",
    "        \n",
    "    #extract email within script tag using regex\n",
    "    regex = re.compile('([\\w\\d\\.!#$%&\\'\\*\\+\\-\\/=\\?\\^_`{|}~;]+@[\\w\\d\\-]+\\.[\\w]{2,})')\n",
    "    \n",
    "    try:\n",
    "        script_content = company_content.find('div',{'class' : 'ListingPageNameAddress NONE'}).script.get_text()\n",
    "    except:\n",
    "        script_content = None\n",
    "    \n",
    "    try:\n",
    "        email = re.search(regex, script_content).group(0)\n",
    "    except:\n",
    "        email = None\n",
    "        \n",
    "    return [str(name), str(phone), str(email)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Over here, I'm looping through the list of catergory links constructed earlier and calling the main scraping function extract_listings, all starting with page 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "company_list = []\n",
    "for page in category_links:\n",
    "    company_info = extract_listings(page,1)\n",
    "    if company_info:\n",
    "        company_list.extend(company_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First 15 of final list containing all companies on the site and their information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['None', '866-891-3863', 'None'], ['A 1 Termite & Pest Control Inc', '213-388-4506', 'mikemhnam@aol.com'], ['A-1 Fumigation Inc.', '562-866-7535', 'a-1fumigation@verizon.net'], ['Access Exterminator Service, Inc.', '714-630-6310', 'russ@accessext.com'], ['Admiral Pest Control Inc', '562-925-8308', 'jeff@admiralpest.com'], ['Ag-Fume Service Inc', '562-803-0256', 'agfume@flash.net'], ['Algon Exterminating Co', '619-561-1991', 'merry@algonpest.com'], ['Assured Audit Pest Prevention', '909-767-8940', 'joe@assuredaudit.com'], ['BG Inspections & Pest Control', '707-410-7907', 'beetleman@comcast.net'], ['BPC, Inc', '805-650-6828', 'pat@bpcx.com'], ['Brezden Pest Control, Inc.', '805-544-9446', 'sales@brezdenpest.com'], ['Bugman Termite & PC, The', '714-992-1292', 'brian@thebugman.com'], ['C H Boddie Pest Control Inc.', '310-839-9270', 'carlton@boddiepestmanagement.com'], ['Cal State Termite & Pest Ctl', '559-896-9320', 'None'], ['California Pest Control', '209-992-1190', 'calpestcontrol@gmail.com']]\n"
     ]
    }
   ],
   "source": [
    "print company_list[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataframe with final list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>company_name</th>\n",
       "      <th>phone</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>866-891-3863</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A 1 Termite &amp; Pest Control Inc</td>\n",
       "      <td>213-388-4506</td>\n",
       "      <td>mikemhnam@aol.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A-1 Fumigation Inc.</td>\n",
       "      <td>562-866-7535</td>\n",
       "      <td>a-1fumigation@verizon.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Access Exterminator Service, Inc.</td>\n",
       "      <td>714-630-6310</td>\n",
       "      <td>russ@accessext.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Admiral Pest Control Inc</td>\n",
       "      <td>562-925-8308</td>\n",
       "      <td>jeff@admiralpest.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        company_name         phone                      email\n",
       "0                               None  866-891-3863                       None\n",
       "1     A 1 Termite & Pest Control Inc  213-388-4506          mikemhnam@aol.com\n",
       "2                A-1 Fumigation Inc.  562-866-7535  a-1fumigation@verizon.net\n",
       "3  Access Exterminator Service, Inc.  714-630-6310         russ@accessext.com\n",
       "4           Admiral Pest Control Inc  562-925-8308       jeff@admiralpest.com"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = ['company_name', 'phone', 'email']\n",
    "df = pd.DataFrame(company_list, columns=columns)\n",
    "\n",
    "#remove return line code in couple of company names \n",
    "df['company_name'] = df['company_name'].replace(to_replace='\\r', value='', regex=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Write data into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(path_or_buf='pest_control_companies.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
